# -- coding: utf-8 --
# @time :
# @author : shajiu
# @email : 18810979033@163.com
# @file : .py
# @software: pycharm

"""
åŠŸèƒ½:ä¸»è¦ç”¨äºåŠ è½½æ¨¡å‹å¹¶åšæ¨ç†
"""
import argparse
import json, os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import GenerationConfig
from transformers import BitsAndBytesConfig

"""
æ”¯æŒæ¨ç†é¢„æµ‹éƒ¨åˆ†çš„ä»£ç 
"""
DEFAULT_SYSTEM_PROMPT = """You are a helpful assistant. ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚"""
system_format = '<|start_header_id|>system<|end_header_id|>\n\n{content}<|eot_id|>'
user_format = '<|start_header_id|>user<|end_header_id|>\n\n{content}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n'
assistant_format = '{content}<|eot_id|>'

parser = argparse.ArgumentParser()
parser.add_argument('--base_model', default=None, type=str, required=True,
                    help="å­˜æ”¾HFæ ¼å¼çš„Llama-3-Chinese-Instructæ¨¡å‹æƒé‡å’Œé…ç½®æ–‡ä»¶çš„ç›®å½•ã€‚ä¹Ÿå¯ä½¿ç”¨ğŸ¤—Model Hubæ¨¡å‹è°ƒç”¨åç§°")
parser.add_argument('--tokenizer_path', default=None, type=str, help="å­˜æ”¾å¯¹åº”tokenizerçš„ç›®å½•ã€‚è‹¥ä¸æä¾›æ­¤å‚æ•°ï¼Œåˆ™å…¶é»˜è®¤å€¼ä¸--base_modelç›¸åŒ")
parser.add_argument('--data_file', default=None, type=str, help="éäº¤äº’æ–¹å¼å¯åŠ¨ä¸‹ï¼ŒæŒ‰è¡Œè¯»å–file_nameä¸­çš„çš„å†…å®¹è¿›è¡Œé¢„æµ‹")
parser.add_argument('--with_prompt', action='store_true',
                    help="æ˜¯å¦å°†è¾“å…¥ä¸promptæ¨¡ç‰ˆè¿›è¡Œåˆå¹¶ã€‚å¦‚æœåŠ è½½Llama-3-Chinese-instructæ¨¡å‹ï¼Œè¯·åŠ¡å¿…å¯ç”¨æ­¤é€‰é¡¹ï¼")
parser.add_argument('--interactive', action='store_true', help="ä»¥äº¤äº’æ–¹å¼å¯åŠ¨ï¼Œä»¥ä¾¿è¿›è¡Œå¤šæ¬¡å•è½®é—®ç­”ï¼ˆæ­¤å¤„ä¸æ˜¯llama.cppä¸­çš„ä¸Šä¸‹æ–‡å¯¹è¯ï¼‰")
parser.add_argument('--predictions_file', default='./predictions.json', type=str,
                    help="éäº¤äº’å¼æ–¹å¼ä¸‹ï¼Œå°†é¢„æµ‹çš„ç»“æœä»¥jsonæ ¼å¼å†™å…¥file_name")
parser.add_argument('--gpus', default="0", type=str, help="æŒ‡å®šä½¿ç”¨çš„GPUè®¾å¤‡ç¼–å·ï¼Œé»˜è®¤ä¸º0ã€‚å¦‚ä½¿ç”¨å¤šå¼ GPUï¼Œä»¥é€—å·åˆ†éš”ï¼Œå¦‚0,1,2")
parser.add_argument('--only_cpu', action='store_true', help='ä»…ä½¿ç”¨CPUè¿›è¡Œæ¨ç†')
parser.add_argument('--load_in_8bit', action='store_true', help="ä½¿ç”¨8bitæ–¹å¼åŠ è½½æ¨¡å‹ï¼Œé™ä½æ˜¾å­˜å ç”¨")
parser.add_argument('--load_in_4bit', action='store_true', help="ä½¿ç”¨4bitæ–¹å¼åŠ è½½æ¨¡å‹ï¼Œé™ä½æ˜¾å­˜å ç”¨ï¼Œæ¨èä½¿ç”¨--load_in_4bit")
parser.add_argument("--use_vllm", action='store_true', help="ä½¿ç”¨vLLMä½œä¸ºLLMåç«¯è¿›è¡Œæ¨ç†")
parser.add_argument('--use_flash_attention_2', action='store_true', help="ä½¿ç”¨Flash-Attention 2åŠ é€Ÿæ¨ç†ï¼Œå¦‚æœä¸æŒ‡å®šè¯¥å‚æ•°ï¼Œä»£ç é»˜è®¤SDPAåŠ é€Ÿã€‚")
args = parser.parse_args()

# å¯ä»¥ä½¿ç”¨vLLMä½œä¸ºLLMåç«¯è¿›è¡Œæ¨ç†
if args.use_vllm:
    # ä½¿ç”¨8bitæˆ–4bitæ–¹å¼åŠ è½½æ¨¡å‹ï¼Œé™ä½æ˜¾å­˜å ç”¨
    if args.load_in_8bit or args.load_in_4bit:
        raise ValueError("vLLM currently does not support quantization, please use fp16 (default) or unuse --use_vllm.")
    # ä»…ä»…ä½¿ç”¨CPU
    if args.only_cpu:
        raise ValueError(
            "vLLM requires GPUs with compute capability not less than 7.0. If you want to run only on CPU, please unuse --use_vllm.")
# æœªä½¿ç”¨vLLMçš„å‰æä¸‹é€šè¿‡ ä½¿ç”¨8bitæˆ–4bitæ–¹å¼åŠ è½½æ¨¡å‹ï¼Œé™ä½æ˜¾å­˜å ç”¨
if args.load_in_8bit and args.load_in_4bit:
    raise ValueError("Only one quantization method can be chosen for inference. Please check your arguments")

# ç›´æ¥ä½¿ç”¨CPUåŠ è½½
if args.only_cpu is True:
    args.gpus = ""  # è®¾ç½®GPUç¼–å·ä¸ºç©º
    # æ­¤æ—¶ï¼Œæ— æ³•é€šè¿‡ä½¿ç”¨8bitæˆ–4bitæ–¹å¼åŠ è½½æ¨¡å‹
    if args.load_in_8bit or args.load_in_4bit:
        raise ValueError("Quantization is unavailable on CPU.")

# è®¾ç½®CUDAå¯è§çš„GPUè®¾å¤‡ç¼–å·
os.environ["CUDA_VISIBLE_DEVICES"] = args.gpus

# è‹¥ä½¿ç”¨vLLMä½œä¸ºLLMåç«¯è¿›è¡Œæ¨ç†ï¼Œåˆ™éœ€è¦å¯¼å…¥å¯¹åº”çš„åŒ…
if args.use_vllm:
    from vllm import LLM, SamplingParams

if args.use_vllm:
    # æ„å»ºä¸€ä¸ªç”Ÿæˆçš„å‚æ•°
    generation_config = dict(
        temperature=0.2,
        top_k=40,
        top_p=0.9,
        max_tokens=100,
        presence_penalty=1.0,
    )
else:
    # ä¸ä½¿ç”¨vLLMä½œä¸ºLLMåç«¯è¿›è¡Œæ¨ç†æ—¶çš„è®¾ç½®
    generation_config = GenerationConfig(
        temperature=0.2,
        top_k=40,
        top_p=0.9,
        do_sample=True,
        num_beams=1,
        repetition_penalty=1.1,
        max_new_tokens=400
    )

# æµ‹è¯•ç”¨ä¾‹
sample_data = ["ä¸ºä»€ä¹ˆè¦å‡å°‘æ±¡æŸ“ï¼Œä¿æŠ¤ç¯å¢ƒï¼Ÿ"]


def generate_prompt(instruction):
    # æ‹¼æ¥æŒ‡ä»¤æ¨¡æ¿
    return system_format.format(content=DEFAULT_SYSTEM_PROMPT) + user_format.format(content=instruction)


if __name__ == '__main__':
    load_type = torch.float16

    # å¦‚æœå¯ç”¨ï¼Œå°†æ¨¡å‹ç§»åŠ¨åˆ°MPSè®¾å¤‡
    if torch.backends.mps.is_available():
        device = torch.device("mps")
    else:
        # GPUå¯ç”¨
        if torch.cuda.is_available():
            device = torch.device(0)
        else:
            # åªèƒ½CPUæ¨ç†
            device = torch.device('cpu')
    print(f"Using device: {device}")

    if args.tokenizer_path is None:
        # æœªæŒ‡å®štokenizerè·¯å¾„åˆ™é»˜è®¤ä½¿ç”¨æ¨¡å‹è·¯å¾„
        args.tokenizer_path = args.base_model
    # åŠ è½½æ¬¡å…ƒåŒ–è·¯å¾„
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)

    # è·å–å¯¹åº”çš„ç´¢å¼•=[2,0]
    terminators = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids("<|eot_id|>")
    ]
    # å¯ä»¥ä½¿ç”¨vLLMä½œä¸ºLLMåç«¯è¿›è¡Œæ¨ç†
    if args.use_vllm:
        # åŠ è½½æ¨¡å‹ã€tokenizeråœ°å€ã€GPUä¸ªæ•°ã€torch.float16
        model = LLM(model=args.base_model,
                    tokenizer=args.tokenizer_path,
                    tensor_parallel_size=len(args.gpus.split(',')),
                    dtype=load_type
                    )
        # åœ¨ä½¿ç”¨vLLMä½œä¸ºLLMåç«¯è¿›è¡Œæ¨ç†æ—¶çš„è®¾ç½®
        generation_config["stop_token_ids"] = terminators
        generation_config["stop"] = ["<|eot_id|>", "<|end_of_text|>"]

    else:
        # éä½¿ç”¨vLLMä½œä¸ºLLMåç«¯è¿›è¡Œæ¨ç†æ—¶çš„è®¾ç½®
        if args.load_in_4bit or args.load_in_8bit:
            # ä½¿ç”¨ NF4 é‡åŒ–åŠ è½½ 4 ä½æ¨¡å‹
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=args.load_in_4bit,
                load_in_8bit=args.load_in_8bit,
                bnb_4bit_compute_dtype=load_type,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )

        model = AutoModelForCausalLM.from_pretrained(
            args.base_model,
            torch_dtype=load_type,
            low_cpu_mem_usage=True,
            device_map='auto',
            quantization_config=quantization_config if (args.load_in_4bit or args.load_in_8bit) else None,
            # ä½¿ç”¨Flash-Attention 2åŠ é€Ÿæ¨ç†
            attn_implementation="flash_attention_2" if args.use_flash_attention_2 else "sdpa"
        )
        # åˆ¤æ–­æ˜¯å¦ä¸ºCPUä¸ŠåŠ è½½
        if device == torch.device('cpu'):
            # å°†æ¨¡å‹çš„æ‰€æœ‰æµ®ç‚¹å‚æ•°å’Œç¼“å†²çš„ç±»å‹è½¬æ¢ä¸ºfloatæ•°æ®ç±»å‹
            model.float()
        model.eval()

    # åˆ¤æ–­æ˜¯å¦åŠ è½½äº†æµ‹è¯•æ•°æ®é›†
    if args.data_file is None:
        examples = sample_data
    else:
        # è¯»å–æµ‹è¯•æ•°æ®é›†(ä¸€è¡Œä¸€ä¸ªé—®é¢˜))
        with open(args.data_file, 'r') as f:
            examples = [line.strip() for line in f.readlines()]
        # æ˜¾ç¤ºå‰10æ¡æµ‹è¯•é›†(query)
        print("first 10 examples:")
        for example in examples[:10]:
            print(example)
    # åˆ¤æ–­åœ¨PyTorchä¸­æ˜¯å¦ç¦ç”¨æ¢¯åº¦è®¡ç®—ã€‚
    with torch.no_grad():
        # ä»¥äº¤äº’æ–¹å¼å¯åŠ¨
        if args.interactive:
            print("Start inference with instruction mode.")

            print('=' * 85)
            print("+ è¯¥æ¨¡å¼ä¸‹ä»…æ”¯æŒå•è½®é—®ç­”ï¼Œæ— å¤šè½®å¯¹è¯èƒ½åŠ›ã€‚\n"
                  "+ å¦‚è¦è¿›è¡Œå¤šè½®å¯¹è¯ï¼Œè¯·ä½¿ç”¨llama.cpp")
            print('-' * 85)
            print("+ This mode only supports single-turn QA.\n"
                  "+ If you want to experience multi-turn dialogue, please use llama.cpp")
            print('=' * 85)

            while True:
                # è·å–è¾“å…¥
                raw_input_text = input("è¾“å…¥æ‚¨çš„é—®é¢˜:")
                # åˆ¤æ–­è¾“å…¥é•¿åº¦æ˜¯å¦ä¸º0
                if len(raw_input_text.strip()) == 0:
                    break
                # æ˜¯å¦å°†è¾“å…¥ä¸promptæ¨¡ç‰ˆè¿›è¡Œåˆå¹¶
                if args.with_prompt:
                    input_text = generate_prompt(instruction=raw_input_text)
                else:
                    # ç›´æ¥å†™å…¥è¿›è¡Œé¢„æµ‹
                    input_text = raw_input_text

                if args.use_vllm:
                    # å¦‚æœä½¿ç”¨vLLMä½œä¸ºLLMåç«¯è¿›è¡Œæ¨ç†æ—¶çš„è®¾ç½®ã€è¾“å…¥ä¸ºlistçš„æ–‡æœ¬ï¼Œã€‘
                    output = model.generate([input_text], SamplingParams(**generation_config), use_tqdm=False)
                    # è¾“å‡ºå…·ä½“çš„ç»“æœæ–‡æœ¬éƒ¨åˆ†
                    response = output[0].outputs[0].text
                else:
                    # å¦‚æœä¸æ˜¯ä½¿ç”¨vLLMä½œä¸ºLLMåç«¯è¿›è¡Œæ¨ç†æ—¶çš„è®¾ç½®çš„è¯ã€‚
                    inputs = tokenizer(input_text, return_tensors="pt")  # add_special_tokens=False ?
                    generation_output = model.generate(
                        input_ids=inputs["input_ids"].to(device),
                        attention_mask=inputs['attention_mask'].to(device),
                        eos_token_id=terminators,
                        pad_token_id=tokenizer.eos_token_id,
                        generation_config=generation_config
                    )
                    s = generation_output[0]
                    output = tokenizer.decode(s, skip_special_tokens=True)
                    # å¦‚æœä½¿ç”¨äº†æŒ‡ä»¤æ‹¼æ¥è¿™å—éœ€ä»ç»“æœä¸­å»é™¤å‰ç¼€éƒ¨åˆ†
                    if args.with_prompt:
                        # åªè¿”å›ååŠéƒ¨åˆ†çš„å†…å®¹
                        response = output.split("assistant\n\n")[-1].strip()
                    else:
                        # å¦åˆ™å…¨éƒ¨è¿”å›
                        response = output
                # è¿”å›çš„ç»“æœ
                print("Response: ", response)
                print("\n")
        else:
            # å¦‚æœä¸æ˜¯äº¤äº’å¼æ–¹å¼ï¼ˆæ–‡ä»¶çº§åˆ«é¢„æµ‹ï¼‰
            print("Start inference.")
            results = []
            # åˆ¤æ–­æ˜¯å¦ä¸ºVllMåŠ è½½æ¨ç†
            if args.use_vllm:
                # æ‹¼æ¥ä½¿ç”¨äº†æŒ‡ä»¤
                if args.with_prompt is True:
                    inputs = [generate_prompt(example) for example in examples]
                else:
                    inputs = examples
                outputs = model.generate(inputs, SamplingParams(**generation_config))

                for index, (example, output) in enumerate(zip(examples, outputs)):
                    response = output.outputs[0].text
                    print(f"======={index}=======")
                    print(f"Input: {example}\n")
                    print(f"Output: {response}\n")
                    results.append({"Input": example, "Output": response})
            else:
                # ä¸ä½¿ç”¨VLLM
                for index, example in enumerate(examples):
                    # æ‹¼æ¥æŒ‡ä»¤
                    if args.with_prompt:
                        input_text = generate_prompt(instruction=example)
                    else:
                        input_text = example
                    inputs = tokenizer(input_text, return_tensors="pt")  # add_special_tokens=False ?
                    generation_output = model.generate(
                        input_ids=inputs["input_ids"].to(device),
                        attention_mask=inputs['attention_mask'].to(device),
                        eos_token_id=terminators,
                        pad_token_id=tokenizer.eos_token_id,
                        generation_config=generation_config
                    )
                    # æ‹¿å‡ºç»“æœ
                    s = generation_output[0]
                    output = tokenizer.decode(s, skip_special_tokens=True)
                    # æ‹¼æ¥äº†æŒ‡ä»¤/éœ€è¦å»é™¤æŒ‡ä»¤å‰ç¼€
                    if args.with_prompt:
                        response = output.split("assistant\n\n")[1].strip()
                    else:
                        response = output
                    print(f"======={index}=======")
                    print(f"Input: {example}\n")
                    print(f"Output: {response}\n")

                    results.append({"Input": input_text, "Output": response})
            # ç»“æœå†™å…¥
            dirname = os.path.dirname(args.predictions_file)
            # åˆ›å»ºæ–‡ä»¶
            os.makedirs(dirname, exist_ok=True)
            # å†™æ–‡ä»¶
            with open(args.predictions_file, 'w') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            # å¦‚æœå¼€å¯äº†vllmåˆ™ä¿å­˜æ­¤é…ç½®é¡¹ç›®
            if args.use_vllm:
                with open(dirname + '/generation_config.json', 'w') as f:
                    json.dump(generation_config, f, ensure_ascii=False, indent=2)
            else:
                # å­˜å‚¨é…ç½®æ–‡ä»¶
                generation_config.save_pretrained('./')
